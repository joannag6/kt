\documentclass[11pt]{article}
\usepackage{colacl}
\sloppy



\title{COMP90049 Report: Analysis of Spelling Correction Methods}
\author
{Anonymous}


%\bibliographystyle{acl}
%\bibliography{sample}

\begin{document}
\maketitle


%\begin{abstract}
%This is a \LaTeX\ sample for your paper.
%\end{abstract}

\section{Introduction}

This report examines a diverse set of spelling correction techniques and compares their respective performances on a particular dataset from UrbanDictionary. % \cite{JOURNAL:1}.
Spelling correction can be treated as an approximate string matching problem. % cite Hall and Dowling

%Regardless of whether the spelling mistakes were due to legitimate spelling variations or a simple typo. %Approximate string matching is a common problem with various real world applications. 

%- include stats of misspelled words in general 
%- include history of spell checkers / spelling correction 

%aims to apply multiple spelling correction methods on a given data set. The methods are then analysed and compared based on their respective performances.

\section{Dataset}
The data consisted of misspelled words, the correct spellings of the respective words, and a "dictionary" of correctly spelled words. These words were identified from UrbanDictionary by Saphra and Lopez (2016). The words are formed from Latin characters but are not necessarily English words. 

This dataset has several deficiencies, which could produce false negatives or false positives in the results. For example, "guilford" (a "correct" entry) doesn't appear in dictionary but "guildford" (a "misspelled" entry) does. The following table collates these shortcomings:


\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|}

\hline
misspelled entries in dictionary & \textit{171}  \\ 
correct entries \textbf{not} in dictionary & \textit{108} \\ 
correct entries same as misspelled ones & \textit{9} \\
\hline

\end{tabular}
\caption{Deficiencies in the provided dataset}\label{deficiencies}
 \end{center}
\end{table}

Further calculation revealed that 275 (out of 716) words are in some way "deficient". This is close to 38.4\% of the data. Since this is such a high percentage, precautions should be taken before making any critical conclusions from this study. % reword

%"a number of headwords taken from UrbanDictionary that have been automatically identified as being misspelled (Saphra and Lopez, 2016)."
%all the words are "english" and are latin characters. however they are not necessarily English which is why we use the provided dictionary  as reference.
%explain how it is not "clean". give some numbers related to this.

%number of deficient data in dataset: 275 (out of 716) -- 38.4\% of data is ?dirty?.

% use terms "False positive" and "False negative"

\section{Methodology}
Three different approximate string matching approaches were used for this problem. This was done in order to get a broader scope of results and to appreciate the respective strengths and weaknesses of each technique.

%to incorporate the advantages from each technique into a singular better solution for this problem


\subsection{Global Edit Distance}
The Needleman-Wunsch Algorithm % CITE@!!
was used to calculate the Global Edit Distance (GED) between the misspelled word and a word in the given dictionary. It returns the maximum possible "score" which represents the GED between the two words. This dynamic-programming algorithm was selected in order to improve the run time complexity of the otherwise lengthy process. Regardless, this program took the longest to execute (i.e., approximately 7 hours for only 716 words).

A simple \texttt{Python} script was used to execute the algorithm, calculating the GED between each word in the given file of misspelled words and each word in the dictionary, returning a list of dictionary words with the largest GED from each respective misspelled word.

\subsubsection{Parameters}
After careful consideration, the parameters \((m, i, d, r) = (+1, -1, -1, -1)\) were chosen over the Levenshtein distance parameters \((m, i, d, r) = (0, -1, -1, -1)\) % CITE@!!
. With these parameters, the difference between a match and a non-match character is further exaggerated. This could be useful when searching for "similar" strings.

%\subsubsection{Comparison with Local Edit Distance}
%Local Edit Distance (LED) is a ... used for  more appropriate for substring matching, but due to the nature of this problem, it will not be very effective in getting good solutions.



\subsection{Neighbourhood Search}
\subsubsection{Implementation with agrep}
This search algorithm was implemented with a \texttt{bash} script which utilised the UNIX command-line utility \texttt{agrep}  % CITE@!!
 due to its impressive efficiency. The code used the \texttt{-\#} flag in the \texttt{agrep} command in order to define the neighbourhood limit, \texttt{k}. \texttt{agrep} was executed on each misspelled word with \texttt{k}-values starting from \texttt{0} and incremented by \texttt{1}, until at least one match from the dictionary (a neighbour) was found.

All the matches for that specific misspelled word were then outputted to standard output. This allowed for each misspelled entry to have at least one result without requiring preprocessing to determine the smallest value of \texttt{k} needed to guarantee a match.

% write more stuff here???

\subsection{Phonetic Matching}
% Given that the problem space involves 

\subsubsection{Implementation with Soundex}
The phonetic matching algorithm was implemented with a \texttt{Python} program, which utilized Soundex % ODELL,M. K., AND RUSSELL, R.C. U,S. Patent nos. 1,261,167(1918) and 1,435,683 (1922)
to encode each word in the dictionary into a four-character code. This encoding is meant to be shared amongst "like-sounding" words. % Hall & Dowling, Approximate String Matching
The program maps each encoding to the list of words that translate to it. Then, the corresponding list for each given misspelled word is returned. It is possible for an empty list to be returned. Given that all the dictionary information is cached, the overall time complexity is very efficient.

\subsubsection{Truncation}
It was decided that the truncation of Soundex codes to four characters should be kept in order to retain the simplicity of exact indexing. Having to account for "similar" codes introduces another approximate string matching problem where additional techniques (eg. edit distance) would be required. Even though it may give a better solution, this could introduce unnecessary complexity instead. % cite Zobel?



\section{Evaluation Metrics} 
In the initial stages of the project, all methods returned a single answer chosen at random from the pool of appropriate solutions. This was repeated 10 times and the accuracy for each run was calculated. (see Table~\ref{accuracies}).

\begin{table}[hb]
\begin{center}
\begin{tabular}{|c|ccc|}

\hline
\# & GED & \texttt{agrep} & Soundex  \\ 
 \hline

1 & 85 & 110 & 2  \\ 
2 & 80 & 105 & 3 \\ 
3 & 89 & 115 & 3 \\
4 & 83 & 107 & 4 \\ 
5 & 84 & 118 & 6 \\
6 & 79 & 106 & 4 \\
7 & 87 & 102 & 4 \\
8 & 83 & 109 & 7 \\
9 & 84 & 110 & 2 \\
10 & 85 & 109 & 4 \\

\hline
Average & 83.9 & 109.1 & 3.9 \\
\hline
\end{tabular}
\caption{Accuracies of each method over 10 runs}\label{accuracies}
 \end{center}
\end{table}

Due to the low accuracy results (only 0.5\% - 15.2\% of the total words) and consistent presence of random factor(s), it was difficult to reliably compare the respective algorithms based on accuracy.

Thus, precision and recall were more appropriate evaluation metrics since they are designed to work on lists of results rather than just one. The algorithms were then amended accordingly. This reduced the unaccountable variation in results. % english this right

%Also calculated the accuracy for each algorithm, where the chosen word is a random answer from the list of answers.Values were like -- insert table --, which is -- insert percentage --, which is quite low. Because it is quite low and the random picking of answers is not very reliable, decided to return a list instead and do analysis on that. In that case,

\subsection{Precision}
Precision is determined by calculating 
\begin{equation}\frac{|correct|}{|attempted|}\end{equation}
where \(correct\) is the correct words the method obtained and \(attempted\) is all of the obtained words. It is used to determine the "proportion of retrieved records that are relevant". %[SALT68, PAIC77]
% Perry, Kent & Berry (1955)

%Determining the precision is useful because even though a technique may return the correct answer, that solution is useless if many wrong answers are returned as well. It is difficult to discern the actual desired solution from such a large pool.

A technique that returns not only correct answers, but also many wrong answers is inadequate. It would be difficult to determine an actual relevant solution. Thus, determining precision is more useful in this regard.

\subsection{Recall}
Recall is determined by calculating 
\begin{equation}\frac{|correct|}{|total|}\end{equation}
where \(correct\) is the correct words the method obtained and \(total\) is the misspelled words. It evaluates the "proportion of relevant records actually retrieved". %[SALT68, PAIC77]

Recall is a method of verifying if a list of solutions contains the right answer at all. If the precision of a procedure is low but it has a high recall, perhaps additional post processing could filter through the results and determine the most relevant solution.

\section{Results}

A \texttt{Python} script was written to calculate the precision and recall of each approximate string matching method.

\begin{table}[h]
 \begin{center}
\begin{tabular}{|c||c|c|c|}

\hline
Method & Precision & Recall \\
\hline\hline
%\multirow{3}{4em}{Multiple row} & cell2 & cell3 \\ 
GED & 0.0481378262 & 0.2653631285 \\ 
\texttt{agrep} & 0.0457670043 & 0.3533519553 \\ 
Soundex & 0.0029336621 & 0.5949720670 \\
\hline

\end{tabular}
\caption{Results for each method, approximated to 10 decimal places}\label{results}
 \end{center}
\end{table}

\section{Analysis}
Regardless of which evaluation metric was used, the results imply that more than half of the solutions would be incorrect. This proves unsatisfactory for any real world applications. 

%In this study, Soundex obtained the highest recall but the lowest precision out of the three methods. These results seem to indicate high numbers of false positives for Soundex. This may be a consequence of truncating the Soundex codes. % can i include reason in this section???

%The method with the highest precision is the Global Edit Distance. Interestingly, it also acquired the lowest recall. This implies that ... %\ why? - implementation

\subsection{GED vs \texttt{agrep}}
GED has slightly better precision than \texttt{agrep}, but only by approximately 5.18\%. However, the recall of \texttt{agrep} is \(33.16\%\) better than GED recall, which is a significant difference. If an algorithm that assists in ranking relevance of all results could be applied, it is possible to obtain the "correct" answer, rendering the difference negligible.% reword
 
% This could affect a large data set -- which is relevant for real world application

% wrt keep using smaller value for denominator and keep to it.
% maybe just use a ratio lol
 
\subsection{GED vs Soundex}
In this study, Soundex obtained the highest recall but the lowest precision out of the three methods, with a recall value 2.24 times better than that of GED. These results seem to indicate high numbers of false positives for Soundex. This may be a consequence of truncating the Soundex codes.
On the other hand, the method with the highest precision is GED. It obtained a precision approximately 16.4 times better than Soundex. Interestingly, it also acquired the lowest recall. %\ why? - implementation

Choosing the right algorithm for a problem would require determining a suitable trade-off between precision and recall. In situations where false positives may be tolerated better, Soundex would be the better option. Additional techniques like using a similarity metric could be used to improve the precision of Soundex. % cite Improving Precision and Recall for Soundex Retrieval, David Holmes and M. Catherine McCabe
However, if there is no reliable way of filtering through the results, GED is more likely to return a correct answer.

\subsection{\texttt{agrep} vs Soundex}
\texttt{agrep} is significantly more precise than Soundex (i.e., 15.6 times better). Recall value of Soundex is only 1.68 times better than \texttt{agrep}. 

This infers that even though Soundex is more likely to have the "correct" answer in its list of solutions, there is a much lower chance of obtaining the actual answer compared to using \texttt{agrep}, provided the selection of final answer is randomised. Unless a sufficiently efficient algorithm that could rank the relevance of all results is available, the Soundex algorithm is likely to be least applicable in real world situations.


%That would most likely make the most difference in a large dataset, maybe with some extra information that could provide some context to the problem. % reword AF

\section{Conclusions}
In conclusion,

Given that all three methods do not necessitate any user input, perhaps some modifications could be applied to make these algorithms user-assisted.  

%Include discussion + possible improvements
%Talk about how all of these algorithms are quite naive and do not include any user input, which is quite important in the context of spelling correction (give an example as well as a reference). Some context could be the whole sentence typed by the user - what is grammatically correct? Also use user data to determine which words are more commonly followed by which others.


%Text,\footnote{some Footnote text} with footnotes at bottom of page. Text of the subsection with citations such as \newcite{Spa72}, \newcite{Kay86} and \newcite{MosWal64}. Note that the citation style is defined in the accompanying style file; it is similar to AAAI house style. You may use other (formal) citation styles if you prefer. Text of the (see Table~\ref{results}).


%Soundex - use better code values?
%agrep is a nice middle ground thing


\end{document}
